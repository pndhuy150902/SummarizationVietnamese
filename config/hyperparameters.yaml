processed_data:
  train_data: "../dataset/full_train_data_summarization.csv"
  train_data_with_title: "../dataset/full_train_data_title_summarization.csv"
  test_data: "../dataset/full_test_data_summarization.csv"

model_mixtral: "mistralai/Mixtral-8x7B-Instruct-v0.1"
model_mistral: "mistralai/Mistral-7B-Instruct-v0.2"

args_training:
  train_batch_size: 256
  eval_batch_size: 32
  num_train_epochs: 2
  learning_rate: 2e-5
  weight_decay: 0.1
  lr_scheduler_type: "cosine"
  gradient_accumulation_steps: 128
  gradient_checkpointing: true
  save_total_limit: 2
  logging_steps: 20
  auto_find_batch_size: true
  load_best_model_at_end: true
  bf16: true
  group_by_length: true
  dir_checkpoint: "./model_checkpoint"
  save_strategy: "epoch"
  evaluation_strategy: "no"
  optimizer: "paged_lion_8bit"
  report_to: "wandb"
  run_name: "Mistral-7B-Instruct-v0.2-QLoRA"
  dataset_num_proc: 8

peft_dir:
  peft_model: './peft_model'

deepspeed:
  stage_2: "../config/deepspeed_stage_2.json"
  stage_2_offload: "../config/deepspeed_stage_2_offload.json"
  stage_3: "../config/deepspeed_stage_3.json"
  stage_3_offload: "../config/deepspeed_stage_3_offload.json"

length:
  text: 4096
  summary: 1024
