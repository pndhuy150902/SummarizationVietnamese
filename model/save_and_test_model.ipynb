{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb13e299",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import gc\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d791c100",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x2867e3268b0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import evaluate\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from peft import PeftModel, PeftConfig\n",
    "from transformers import BitsAndBytesConfig, AutoTokenizer, AutoModelForCausalLM, DataCollatorForLanguageModeling\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "tqdm.pandas()\n",
    "gc.collect()\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0d38551",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_data_test = pd.read_csv('../dataset/full_test_data_summarization.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8ffe0d7e5ede1fea",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "checkpoint = './model_checkpoint/checkpoint-1576'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b213263",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = PeftConfig.from_pretrained(checkpoint)\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    config.base_model_name_or_path,\n",
    "    device_map={\"\":0},\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    attn_implementation=\"flash_attention_2\",\n",
    "    quantization_config=BitsAndBytesConfig(    \n",
    "        load_in_4bit=True,\n",
    "        load_4bit_use_double_quant=True,\n",
    "        bnb_4bit_quant_type='nf4',\n",
    "        bnb_4bit_compute_dtype=torch.bfloat16\n",
    "    )\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17876766",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PeftModel.from_pretrained(base_model, checkpoint, device_map={\"\":0})\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8de35050220e391d",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def create_prompt(sample):\n",
    "    template = \"\"\"<s>[INST] Bạn là một trợ lý AI. Bạn sẽ được giao một nhiệm vụ. Hãy tóm lược ngắn gọn nội dung sau bằng tiếng Việt:\n",
    "{} [/INST]\"\"\"\n",
    "    prompt = template.format(sample)\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8869c197",
   "metadata": {},
   "outputs": [],
   "source": [
    "references = []\n",
    "predictions = []\n",
    "def generate_text():\n",
    "  for batch_1, batch_2 in tqdm(zip(torch.utils.data.DataLoader(full_data_test['context'], batch_size=112, shuffle=False), torch.utils.data.DataLoader(full_data_test['summarization'], batch_size=112, shuffle=False), strict=True), total=int(round(len(full_data_test)/112, 0))):\n",
    "    prompts = [create_prompt(context) for context in batch_1]\n",
    "    inputs = tokenizer(prompts, add_special_tokens=True, padding=True, return_tensors=\"pt\").to(device)\n",
    "    outputs = model.generate(\n",
    "      **inputs,\n",
    "      early_stopping=False,\n",
    "      max_new_tokens=1024,\n",
    "      temperature=0.7,\n",
    "      top_p=0.8,\n",
    "      top_k=50,\n",
    "      repetition_penalty=1.2,\n",
    "      pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "    references.extend(batch_2)\n",
    "    predictions.extend([tokenizer.decode(output, skip_special_tokens=True, clean_up_tokenization_spaces=False).split('[/INST]')[1].strip() for output in outputs])\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "988564f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "references = []\n",
    "predictions = []\n",
    "def generate_text():\n",
    "  for batch_1, batch_2 in tqdm(zip(torch.utils.data.DataLoader(full_data_test['context'], batch_size=112, shuffle=False), torch.utils.data.DataLoader(full_data_test['summarization'], batch_size=112, shuffle=False), strict=True), total=int(round(len(full_data_test)/112, 0))):\n",
    "    prompts = [create_prompt(context) for context in batch_1]\n",
    "    inputs = tokenizer(prompts, add_special_tokens=True, padding=True, return_tensors=\"pt\").to(device)\n",
    "    outputs = model.generate(\n",
    "      **inputs,\n",
    "      early_stopping=False,\n",
    "      max_new_tokens=1024,\n",
    "      temperature=0.7,\n",
    "      top_k=10,\n",
    "      penalty_alpha=0.8,\n",
    "      repetition_penalty=1.2,\n",
    "      pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "    references.extend(batch_2)\n",
    "    predictions.extend([tokenizer.decode(output, skip_special_tokens=True, clean_up_tokenization_spaces=False).split('[/INST]')[1].strip() for output in outputs])\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9ce43e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "803b40ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_data_test['summarization_predictions'] = predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd11efeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "rouge_metric = evaluate.load(\"rouge\")\n",
    "rouge_scores = rouge_metric.compute(references=references, predictions=predictions, use_stemmer=True, rouge_types=['rouge1', 'rouge2', 'rougeL'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "aaa17e8bd83fd88b",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rouge1': 0.6073709993163005,\n",
       " 'rouge2': 0.3535813077835422,\n",
       " 'rougeL': 0.4224855441941776}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rouge_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bd2930552f0f3689",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "full_data_test.to_csv('test_mistral_lora_v2.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70beee6a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
