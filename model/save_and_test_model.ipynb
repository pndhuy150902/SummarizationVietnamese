{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fb13e299",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import gc\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d791c100",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bin d:\\PythonVenv\\lib\\site-packages\\bitsandbytes\\libbitsandbytes_cuda118.dll\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x29fd3cd6af0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import evaluate\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from peft import PeftModel, PeftConfig\n",
    "from transformers import BitsAndBytesConfig, AutoTokenizer, AutoModelForCausalLM, DataCollatorForLanguageModeling\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "tqdm.pandas()\n",
    "gc.collect()\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0d38551",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_data_test = pd.read_csv('../dataset/full_test_data_summarization.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8ffe0d7e5ede1fea",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "checkpoint = './model_checkpoint/new-checkpoint/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "921f6a95",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = PeftConfig.from_pretrained(checkpoint)\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    config.base_model_name_or_path,\n",
    "    device_map={\"\":0},\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    attn_implementation=\"flash_attention_2\",\n",
    "    quantization_config=BitsAndBytesConfig(    \n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "        bnb_4bit_quant_type='nf4',\n",
    "        bnb_4bit_compute_dtype=torch.bfloat16\n",
    "    ),\n",
    "    token=\"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\"\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "tokenizer.padding_side = \"left\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17876766",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PeftModel.from_pretrained(base_model, checkpoint, device_map={\"\":0})\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b7002ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "qdora_merged = \"\"\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    qdora_merged,\n",
    "    device_map={\"\":0},\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    attn_implementation=\"flash_attention_2\",\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(qdora_merged)\n",
    "tokenizer.padding_side = \"left\"\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8de35050220e391d",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def create_prompt(sample):\n",
    "    template = \"\"\"<s>[INST] Bạn là một trợ lí AI tiếng Việt hữu ích. Bạn hãy tóm lược ngắn gọn nội dung chính của văn bản sau:\n",
    "{} [/INST] \"\"\"\n",
    "    prompt = template.format(sample)\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5fdd22e",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"Ngô Phương Lan được biết đến nhiều hơn khi đăng quang Hoa hậu Thế giới Người Việt năm 2007. Cô là người đẹp xuất thân trong gia đình có truyền thống về ngoại giao. Chính vì vậy, từ khi còn rất nhỏ, Ngô Phương Lan đã mong ước trở thành một nhà ngoại giao tài ba. Được biết, Ngô Phương Lan từng theo cha mẹ sang Mỹ sinh sống và được tổng thống Mỹ Bill Clinton trao tặng bằng khen vì thành tích học tập cao ở trường Tiểu học New York. Trong loạt ảnh thời thơ ấu của Hoa hậu có thể thấy, Ngô Phương Lan sở hữu ngoại hình đáng yêu, nụ cười sáng. Ngô Phương Lan đã tốt nghiệp ngành Quan hệ quốc tế tại đại học Genève (Thuỵ Sĩ) với bản luận văn xuất sắc và được đánh giá rất cao về đề tài văn hoá - lịch sử. Chia sẻ về bức ảnh lúc nhỏ, Hoa hậu Ngô Phương Lan vui vẻ cho biết: \"Chưa đầy 4 tuổi đã đội vương miện và có riêng một đội bodyguards.. mình phục mình quá\". Hình ảnh của Hoa hậu Ngô Phương Lan bên bố mẹ và chị gái cách đây 26 năm. Thời điểm đó, cô đang háo hức cùng gia đình chuẩn bị cho mùa Giáng sinh đầu tiên trên đất Mỹ. Từ nhỏ, Hoa hậu Ngô Phương Lan đã có cuộc sống sung sướng bên bố mẹ. Cô được học hành trong điều kiện tốt. Hoa hậu Ngô Phương Lan bên chồng Tây, chị gái và bố mẹ. Nhan sắc xinh đẹp mặn mà của Hoa hậu Ngô Phương Lan.\"\"\"\n",
    "abstract = \"\"\"Không chỉ sở hữu vẻ xinh xắn, đáng yêu từ nhỏ, Hoa hậu Ngô Phương Lan còn từng được tổng thống Mỹ Bill Clinton trao tặng bằng khen vì thành tích học tập cao ở trường Tiểu học New York.\"\"\"\n",
    "inputs = tokenizer(create_prompt(text), add_special_tokens=True, padding=True, return_tensors=\"pt\").to(device)\n",
    "outputs = model.generate(\n",
    "  **inputs,\n",
    "  early_stopping=False,\n",
    "  max_new_tokens=768,\n",
    "  temperature=0.1,\n",
    "  top_p=0.95,\n",
    "  top_k=40,\n",
    "  repetition_penalty=1.05,\n",
    "  pad_token_id=tokenizer.unk_token_id\n",
    ")\n",
    "tokenizer.decode(outputs[0], skip_special_tokens=True, clean_up_tokenization_spaces=False).split('[/INST]')[1].strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8869c197",
   "metadata": {},
   "outputs": [],
   "source": [
    "references = []\n",
    "predictions = []\n",
    "def generate_text():\n",
    "  for batch_1, batch_2 in tqdm(zip(torch.utils.data.DataLoader(full_data_test['context'], batch_size=32, shuffle=False), torch.utils.data.DataLoader(full_data_test['summarization'], batch_size=32, shuffle=False), strict=True), total=int(round(len(full_data_test)/32, 0))):\n",
    "    prompts = [create_prompt(context) for context in batch_1]\n",
    "    inputs = tokenizer(prompts, add_special_tokens=True, padding=True, return_tensors=\"pt\").to(device)\n",
    "    outputs = model.generate(\n",
    "      **inputs,\n",
    "      early_stopping=False,\n",
    "      max_new_tokens=1024,\n",
    "      temperature=0.7,\n",
    "      top_p=0.8,\n",
    "      top_k=50,\n",
    "      repetition_penalty=1.2,\n",
    "      pad_token_id=tokenizer.unk_token_id\n",
    "    )\n",
    "    references.extend(batch_2)\n",
    "    predictions.extend([tokenizer.decode(output, skip_special_tokens=True, clean_up_tokenization_spaces=False).split('[/INST]')[1].strip() for output in outputs])\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "988564f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "references = []\n",
    "predictions = []\n",
    "def generate_text():\n",
    "  for batch_1, batch_2 in tqdm(zip(torch.utils.data.DataLoader(full_data_test['context'], batch_size=32, shuffle=False), torch.utils.data.DataLoader(full_data_test['summarization'], batch_size=32, shuffle=False), strict=True), total=int(round(len(full_data_test)/32, 0))):\n",
    "    prompts = [create_prompt(context) for context in batch_1]\n",
    "    inputs = tokenizer(prompts, add_special_tokens=True, padding=True, return_tensors=\"pt\").to(device)\n",
    "    outputs = model.generate(\n",
    "      **inputs,\n",
    "      early_stopping=False,\n",
    "      max_new_tokens=1024,\n",
    "      temperature=0.7,\n",
    "      top_k=10,\n",
    "      penalty_alpha=0.8,\n",
    "      repetition_penalty=1.2,\n",
    "      pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "    references.extend(batch_2)\n",
    "    predictions.extend([tokenizer.decode(output, skip_special_tokens=True, clean_up_tokenization_spaces=False).split('[/INST]')[1].strip() for output in outputs])\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9ce43e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "803b40ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_data_test['summarization_predictions'] = predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd11efeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "rouge_metric = evaluate.load(\"rouge\")\n",
    "rouge_scores = rouge_metric.compute(references=references, predictions=predictions, use_stemmer=True, rouge_types=['rouge1', 'rouge2', 'rougeL'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "aaa17e8bd83fd88b",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rouge1': 0.6073709993163005,\n",
       " 'rouge2': 0.3535813077835422,\n",
       " 'rougeL': 0.4224855441941776}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rouge_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bd2930552f0f3689",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "full_data_test.to_csv('test_mistral_lora_v2.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7aee9d4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
